{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingest Darwin Core OBIS ENV formatted data for MBON portal display\n",
    "\n",
    "## An overview\n",
    "\n",
    "The Marine Biodiversity Observation Network portal is based on a multi-layer technology stack.  It involves web design, and of course data science type technical solutions, including but not limited to tools like: geoserver, wms, wfs, and javascript.\n",
    "\n",
    "The data provided by MBON or affiliated researchers is often in a community-held standard, Darwin Core, specifically an application of Darwin Core that enables OBIS's large, interoperable database. \n",
    "\n",
    "But these datasets need to be made table-like and lighter weight for optimal portal-map functionality. It's a balancing act between displaying useful data products, and giving user's a satisfactory level of functionality.\n",
    "\n",
    "## The steps\n",
    "\n",
    "1) Get the data from a provider - sometimes directly, usually through the OBIS endpoint.\n",
    "\n",
    "2) Look at the data and metadata, and together with other stakeholders, determine the visualization goals/requirements of this dataset.\n",
    "\n",
    "3) Download and wrangle the data into 'wide' or 'table-like' shape. (Darwin Core in OBIS-ENV format is 'long' or 'tidy')\n",
    "\n",
    "This involves:\n",
    "\n",
    "    - Determining the 'keys' that link events/occurrence and measurements across the 3 tables of OBIS-ENV format\n",
    "    \n",
    "    - Determining Null data, some data clean up like trimming whitespace or trailing characters, dropping columns unnecessary for visualization.\n",
    "    \n",
    "    - Joining tables, Occurrence, Event and MeasurementOrFact into a single table.\n",
    "    \n",
    "4) Formatting and typing the date-time field and other 'cleaning' peices of the final data table.\n",
    "5) Sending the table to the server that will host it for visualization at a moment's notice on the MBON Portal.\n",
    "6) The map in the data portal performs many client-side tasks as well, that create the final product on the map, tasks like sum, count, or assign a color palette to the 'heatmap' effect.\n",
    "\n",
    " ## This notebook...\n",
    " ... walks through a generic set of steps that represent this process with more detail. This notebook contains almost all the same steps and manipualtions done to the data through it's journey to the map.\n",
    " \n",
    " By: Adrienne Canino, Axiom Data Science  \n",
    " June 1, 2022  \n",
    " Version: 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup environment\n",
    "import pandas as pd # a data wrangling/analytics library\n",
    "import geopandas as gpd # a library for geospatial data transformations\n",
    "from shapely.geometry import Point #another library for geospatial data transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example_OBIS_INGEST.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "#Pull in data - these data are usually stored locally to the notebook, so it's a bit of looking around with ls and cd line commands\n",
    "\n",
    "%ls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangle\n",
    "\n",
    "Each of the occurrence, event and mof tables get pulled into their own data frames (a table for manipulating the data in this analysis environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "occ = pd.read_csv('occurrence.txt', sep=\"\\t\")\n",
    "print(occ.columns)\n",
    "mof = pd.read_csv('extendedmeasurementorfact.txt', sep=\"\\t\")\n",
    "print(mof.columns)\n",
    "event = pd.read_csv('event.txt', sep=\"\\t\")\n",
    "print(event.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at the tops and tails of the data tables, find how many unique values are there\n",
    "\n",
    "I may look for specific values or just poke around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mof['measurementType'].unique())\n",
    "fish_mof.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at the top\n",
    "occ.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking for what measurements are in the dataset\n",
    "mof['measurementType'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking at the observations of a specific measurement\n",
    "mof.loc[mof['measurementType']== \"Biomass\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unerstand the dataframe's shape, nulls, data types, length, etc\n",
    "occ.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for those 'key' ids that should create join-locations across the three tables to one single table\n",
    "\n",
    "Hope for no whammys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occID = occ['id'].unique()\n",
    "eventID = event['id'].unique()\n",
    "mofID = mof['id'].unique()\n",
    "occID == eventID #Trues\n",
    "eventID ==mofID #Trues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin joining tables\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join event and occurrence\n",
    "The event table, and the occurrence table, can be joined together on `occurrenceID` without any further transformation (usually). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#make a new dataframe (df) with occurrence\n",
    "df = occ.set_index('id').join(event.set_index('id'), on=\"id\", rsuffix=\"_event\") \n",
    "# I specified the index to avoid an error about 'type' of columns\n",
    "#now take a look, because just the assigning doesn't give me an output cell\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# I really like to see the shape and all of the dataframe often\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#And I like to see the columns exactly for the next step, dropping columns I will not need for the portal's visualzation\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#I often check if there are more than one values for variables that could be of interest in the visualization,\n",
    "#but if there is only one single unique value, I will often remove the column to reduce load time and complexity of the final map\n",
    "df['waterBody'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trim that intermediary dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Trim the data frame by dropping the columns not needed for visualization.\n",
    "df.drop(columns=['eventID_event','eventDate_event',\n",
    "                'decimalLatitude_event', 'decimalLongitude_event', 'verbatimEventDate','year',\n",
    "                'taxonomicStatus', 'acceptedNameUsage','phylum', 'class','order', 'family', 'genus',\n",
    "                'specificEpithet','scientificNameAuthorship','recordedBy','identifiedBy','identificationRemarks', \n",
    "                'geodeticDatum','coordinateUncertaintyInMeters', 'georeferencedBy','georeferenceProtocol',\n",
    "                'modified', 'language', 'license', 'references', 'dynamicProperties',\n",
    "                 'samplingProtocol', 'sampleSizeValue', 'sampleSizeUnit','dataGeneralizations',\n",
    "                 'institutionID', 'institutionCode', 'datasetName', 'ownerInstitutionCode',\n",
    "                'language', 'license', 'references', 'institutionID', 'waterBody', 'locality',\n",
    "                 'country', 'countryCode', 'stateProvince'\n",
    "                ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pivot the 'long' style MOF table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# taking a look\n",
    "mof.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#checking for expected values to be their own new columns\n",
    "mof['measurementType'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mof['measurementMethod'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I will often also check for specifics, sometimes only to better understand the data I'm trying to manipulate, sometimes to answer questions as I see inconcsistencies or challenges to my final objective\n",
    "mof.loc[mof['measurementType']== \"Biomass\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# test the pivot\n",
    "mof.pivot(index='occurrenceID', columns='measurementType', values='measurementValue')\n",
    "#if that looks like what I want, assign it to a new dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#assign the pivoted, or 'wide', table to a dataframe of it's own\n",
    "mofDF = fish_mof.pivot(index='occurrenceID', columns='measurementType', values='measurementValue')\n",
    "mofDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset the column names and index and everything so it's a well behaved data frame\n",
    "mofDF = mofDF.rename_axis(None, axis=1).reset_index('occurrenceID')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check that it worked\n",
    "mofDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join the intermediary data frames together\n",
    "All wide tables now making one big wide table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df.join(mofDF.set_index('occurrenceID'), on=\"occurrenceID\")\n",
    "df.head()\n",
    "#boom all three joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#get my columns again, getting ready to trim the tables\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up for final visualization\n",
    "\n",
    "Listing and determining the final columns I'll need for visualizing these data on the portal map.\n",
    "It's usually a list like this:\n",
    "* scientificName \n",
    "* aphiaID\n",
    "* lat/lon GEOM\n",
    "* eventID\n",
    "* eventDate\n",
    "* occurrenceID\n",
    "* occurrenceStatus\n",
    "* measurements\n",
    "   - Biomass\n",
    "   - Size\n",
    "\n",
    "Sometimes there are additional elements of interest, depending on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim the data frame again down to the very basics\n",
    "df = df.drop(columns=['basisOfRecord', 'recordedBy', 'individualCount', 'decimalLatitude',\n",
    "       'decimalLongitude', 'identifiedBy', 'identificationRemarks',\n",
    "       'scientificNameID', 'acceptedNameUsage', 'kingdom',\n",
    "       'phylum', 'class', 'order', 'family', 'genus', 'specificEpithet',\n",
    "       'scientificNameAuthorship', 'vernacularName', 'taxonomicStatus', 'type',\n",
    "       'modified', 'language', 'license', 'references', 'institutionID',\n",
    "       'datasetID', 'institutionCode', 'datasetName', 'ownerInstitutionCode',\n",
    "       'dataGeneralizations', 'dynamicProperties', 'eventID_event',\n",
    "       'samplingProtocol', 'sampleSizeValue', 'sampleSizeUnit',\n",
    "       'eventDate_event', 'year', 'verbatimEventDate', 'habitat', 'locationID',\n",
    "       'waterBody', 'islandGroup', 'country', 'countryCode',\n",
    "       'stateProvince', 'locality', 'minimumDepthInMeters',\n",
    "       'maximumDepthInMeters', 'geodeticDatum',\n",
    "       'coordinateUncertaintyInMeters', 'georeferencedBy',\n",
    "       'georeferenceProtocol', 'Consumer Type', 'LW_a', 'LW_b', 'Size Bin', 'Trophic Level'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#clean up taxonID column\n",
    "df['taxonID'] = df['taxonID'].str.lstrip('aphiaID_')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sometimes check and address any value inconsistencies\n",
    "df['occurrenceStatus'].unique() #there are no caps or etc to mess with category. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check to data types to be correct, most particularly datetimes.\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create geo- dataframe\n",
    "\n",
    "A dataframe that plays nicely with geospatial data and technologies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#build geometry part\n",
    "df['geometry'] = list(zip(df['decimalLongitude_event'], df['decimalLatitude_event']))\n",
    "df['geometry'] = df['geometry'].apply(Point)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make geodataframe out of the table, with CRS and geometry explicit\n",
    "gdf = gpd.GeoDataFrame(df, crs = 'EPSG:4326', geometry='geometry')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unecessary columns, address any date typing necessary\n",
    "gdf = gdf.drop(columns=['decimalLongitude_event', 'decimalLatitude_event'])\n",
    "gdf = gdf.astype({'eventDate':'datetime64[ns]'})\n",
    "gdf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I often save a csv of the table I send to the portal's server for reference in future parts of the process\n",
    "gdf.to_csv(\"New_MBON_data_Geoserver.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#push the table to geoserver database with the right layer name and details\n",
    "\n",
    "from sqlalchemy import create_engine #I need one more piece of a library to play nice with sql type databases\n",
    "\n",
    "db_url = 'superURL'\n",
    "\n",
    "#create_engine from sqlalchemy library\n",
    "#connects our postgres sql database via its url\n",
    "engine = create_engine(db_url)\n",
    "\n",
    "\n",
    "#table name in postgres\n",
    "layer_name = 'New_MBON_data_2022'\n",
    "\n",
    "gdf.to_postgis(layer_name, engine, schema='mbon', if_exists='replace', chunksize=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull some metadata "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some details for the geoserver layer etc\n",
    "print(event['decimalLatitude'].min())\n",
    "print(event['decimalLatitude'].max())\n",
    "print(event['decimalLongitude'].min())\n",
    "print(event['decimalLongitude'].max())\n",
    "\n",
    "minDate = df['eventDate'].min()\n",
    "maxDate = df['eventDate'].max()\n",
    "print(\"min=\"+minDate)\n",
    "print(\"max=\"+maxDate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data have been transformed into a light weight table of select variables for visualizing on the portal map, the next steps are to update and attach the Portal catalog information to the server's back end. Then the 'skinning' of the visualization with json and javascript and other more front-end web tools is created, to enable filters, data charts, hover-over features, etc. on the MBON portal. \n",
    "\n",
    "Check out the MBON portal for more here: [https://mbon.ioos.us/?ls=s3ShXuOX](https://mbon.ioos.us/?ls=s3ShXuOX)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}